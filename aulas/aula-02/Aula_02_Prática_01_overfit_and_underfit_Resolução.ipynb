{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Aula 02 - Prática 01 - overfit_and_underfit.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fTFj8ft5dlbS"
      },
      "source": [
        "##### Copyright 2018 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "lzyBOpYMdp3F",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab_type": "code",
        "id": "m_x4KfSJ7Vt7",
        "colab": {}
      },
      "source": [
        "#@title MIT License\n",
        "#\n",
        "# Copyright (c) 2017 François Chollet\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a\n",
        "# copy of this software and associated documentation files (the \"Software\"),\n",
        "# to deal in the Software without restriction, including without limitation\n",
        "# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n",
        "# and/or sell copies of the Software, and to permit persons to whom the\n",
        "# Software is furnished to do so, subject to the following conditions:\n",
        "#\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n",
        "# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n",
        "# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n",
        "# DEALINGS IN THE SOFTWARE."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGVG7rp0iXdE",
        "colab_type": "text"
      },
      "source": [
        "# Prática 01\n",
        "A intenção desse notebook é mostrar um pouco sobre a etapa de treinamento, focando em alguns pontos sobre _overfitting_ e _underfitting_. É uma versão enxuta do notebook [overfit and underfit](https://www.tensorflow.org/tutorials/keras/overfit_and_underfit) com [Keras](https://www.tensorflow.org/guide/keras)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C9HmC2T4ld5B"
      },
      "source": [
        "# \"Introdução\"\n",
        "Nessa primeira parte iremos apenas importar os pacotes que utilizaremos posteriormente e conhecer o dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pZ8A2liqvgk",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1cweoTiruj8O"
      },
      "source": [
        "## Download do dataset do IMDB\n",
        "\n",
        "Em vez de usar uma incorporação como no bloco de anotações anterior, aqui codificaremos as frases com vários caracteres quentes. Esse modelo superajustará rapidamente o conjunto de treinamento. Ele será usado para demonstrar quando o ajuste excessivo ocorre e como combatê-lo.\n",
        "\n",
        "A codificação múltipla de nossas listas significa transformá-las em vetores de 0s e 1s. Concretamente, isso significaria, por exemplo, transformar a sequência `[3, 5]` em um vetor de 10.000 dimensões que seria todo-zero, exceto os índices 3 e 5, que seriam os mesmos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QpzE4iqZtJly",
        "colab": {}
      },
      "source": [
        "NUM_WORDS = 10000\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n",
        "\n",
        "def multi_hot_sequences(sequences, dimension):\n",
        "    # Cria a matriz de zeros de dimensões (len(sequences), dimension)\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, word_indices in enumerate(sequences):\n",
        "        results[i, word_indices] = 1.0  # atribuindo 1 a indices especificos\n",
        "    return results\n",
        "\n",
        "\n",
        "train_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\n",
        "test_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC50FUAQn53v",
        "colab_type": "text"
      },
      "source": [
        "Discutir técnica em sala: [**One Hot Encoding**](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MzWVeXe3NBTn"
      },
      "source": [
        "Vejamos um dos vetores multi-quentes resultantes. Os índices de palavras são classificados por frequência, portanto, espera-se que haja mais valores 1 próximos ao índice zero, como podemos ver neste gráfico:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "71kr5rG4LkGM",
        "colab": {}
      },
      "source": [
        "plt.plot(train_data[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lglk41MwvU5o"
      },
      "source": [
        "## Demonstrar overfitting\n",
        "\n",
        "No aprendizado profundo, o número de parâmetros aprendíveis em um modelo é frequentemente chamado de \"capacidade\" do modelo.\n",
        "\n",
        "**Debate**:\n",
        "- O que ocorre se a capacidade de memorização do modelo for grande?\n",
        "  - E o que ocorre se essa capacidade for pequena?\n",
        "\n",
        "Iremos agora criar alguns modelos simples para podermos mostrar a ocorrência de _overfitting_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ReKHdC2EgVu"
      },
      "source": [
        "### Baseline model\n",
        "\n",
        "O que estamos usando:\n",
        "\n",
        "- [`keras.Sequential`](https://keras.io/models/sequential/): permite inserir camadas de uma rede neural em série, onde o output da primeira camada serve como input da segunda, e assim por diante.\n",
        "  - [`keras.Sequential.compile()`](https://keras.io/models/sequential/#compile): configura o modelo para treinamento.\n",
        "    - `optimizer`: o otimizador do modelo.\n",
        "    - `loss`: função de perda.\n",
        "    - `metrics`: lista de métricas a serem avaliadas pelo modelo durante o treinamento e o teste.\n",
        "- [`keras.layers.Dense`](https://keras.io/layers/core/#dense): uma camada densa representa uma multiplicação de vetores de matriz.\n",
        "  - `units`: quantidade de neurônios na camada.\n",
        "  - `activation`: função de ativação da camada.\n",
        "  - `input_shape`: dimensão da entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QKgdXPx9usBa",
        "colab": {}
      },
      "source": [
        "# Modelo\n",
        "baseline_model = keras.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Configuramos o modelo para treinamento\n",
        "baseline_model.compile(optimizer='adam',\n",
        "                       loss='binary_crossentropy',\n",
        "                       metrics=['accuracy', 'binary_crossentropy'])\n",
        "# Para visualizarmos o modelo\n",
        "baseline_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rx1kJn8s-T4",
        "colab_type": "text"
      },
      "source": [
        "O que estamos usando:\n",
        "\n",
        "- [`keras.model.fit()`](https://keras.io/models/model/#fit): treina o modelo em um número fixo de épocas.\n",
        "  - `x`: dados inseridos.\n",
        "  - `y`: dados que queremos prever.\n",
        "  - `epochs`: número de épocas para treinar o modelo.\n",
        "  - `batch_size`: número de amostras por atualização de gradiente.\n",
        "  - `validation_data`: dados nos quais avaliar a perda e quaisquer métricas de modelo no final de cada época.\n",
        "  - `verbose`: modo verboso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LqG3MXF5xSjR",
        "colab": {}
      },
      "source": [
        "baseline_history = baseline_model.fit(train_data,\n",
        "                                      train_labels,\n",
        "                                      epochs=20,\n",
        "                                      batch_size=512,\n",
        "                                      validation_data=(test_data, test_labels),\n",
        "                                      verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L-DGRBbGxI6G"
      },
      "source": [
        "### Smaller model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SrfoVQheYSO5"
      },
      "source": [
        "Vamos criar um modelo menor que o baseline model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jksi-XtaxDAh",
        "colab": {}
      },
      "source": [
        "# Modelo\n",
        "smaller_model = keras.Sequential([\n",
        "    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(4, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Configuramos o modelo para treinamento\n",
        "smaller_model.compile(optimizer='adam',\n",
        "                      loss='binary_crossentropy',\n",
        "                      metrics=['accuracy', 'binary_crossentropy'])\n",
        "# Para visualizarmos o modelo\n",
        "smaller_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jbngCZliYdma"
      },
      "source": [
        "Iremos treinar o modelo usando os mesmos parâmetros do anterior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ofn1AwDhx-Fe",
        "colab": {}
      },
      "source": [
        "smaller_history = smaller_model.fit(train_data,\n",
        "                                    train_labels,\n",
        "                                    epochs=20,\n",
        "                                    batch_size=512,\n",
        "                                    validation_data=(test_data, test_labels),\n",
        "                                    verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vIPuf23FFaVn"
      },
      "source": [
        "### Bigger model\n",
        "\n",
        "Agora iremos criar um modelo muito maior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ghQwwqwqvQM9",
        "colab": {}
      },
      "source": [
        "# Modelo\n",
        "bigger_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(512, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Configuramos o modelo para treinamento\n",
        "bigger_model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy','binary_crossentropy'])\n",
        "# Para visualizarmos o modelo\n",
        "bigger_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D-d-i5DaYmr7"
      },
      "source": [
        "Iremos treinar o modelo usando os mesmos parâmetros dos anteriores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U1A99dhqvepf",
        "colab": {}
      },
      "source": [
        "bigger_history = bigger_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fy3CMUZpzH3d"
      },
      "source": [
        "### Visualizar o treinamento e a validação"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HSlo1F4xHuuM"
      },
      "source": [
        "- As linhas sólidas mostram a perda de treinamento.\n",
        "- As linhas tracejadas mostram a perda de validação (lembre-se: uma menor perda de validação indica um modelo melhor)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0XmKDtOWzOpk",
        "colab": {}
      },
      "source": [
        "def plot_history(histories, key='binary_crossentropy'):\n",
        "  plt.figure(figsize=(16,10))\n",
        "\n",
        "  for name, history in histories:\n",
        "    val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                   '--', label=name.title()+' Val')\n",
        "    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "             label=name.title()+' Train')\n",
        "\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel(key.replace('_',' ').title())\n",
        "  plt.legend()\n",
        "\n",
        "  plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "\n",
        "plot_history([('baseline', baseline_history),\n",
        "              ('smaller', smaller_history),\n",
        "              ('bigger', bigger_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Bi6hBhdnSfjA"
      },
      "source": [
        "**Discussão**:\n",
        "- Qual o melhor modelo com capacidade de generalização?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ASdv7nsgEFhx"
      },
      "source": [
        "## Estratégias para evitar o overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4rHoVWcswFLa"
      },
      "source": [
        "### Adicionar regularização de peso"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kRxWepNawbBK"
      },
      "source": [
        "A intenção aqui é tornarmos o nosso modelo mais simples, de forma que a entropia dos dados diminua, como também ele possa focar nos detalhes importantes das caracteristicas que ele esta aprendendo.\n",
        "\n",
        "Para isso, iremos restringir a complexidade da rede, forçando os pesos apenas a aceitar valores pequenos, o que torna a distribuição dos valores de peso mais \"regular\". Aqui podemos usar duas regularizações:\n",
        "\n",
        "- [Regularização de L1](https://developers.google.com/machine-learning/glossary/#L1_regularization), onde o custo adicionado é proporcional ao valor absoluto dos coeficientes de pesos (ou seja, ao que é chamado de \"norma L1\" dos pesos).\n",
        "\n",
        "- [Regularização de L2](https://developers.google.com/machine-learning/glossary/#L2_regularization), onde o custo adicionado é proporcional ao quadrado do valor dos coeficientes de pesos (ou seja, ao que é chamado de \"norma L2 quadrática\" dos pesos). A regularização de L2 também é chamada de redução de peso no contexto de redes neurais. Não deixe que o nome diferente o confunda: a redução de peso é matematicamente a mesma que a regularização de L2.\n",
        "\n",
        "A regularização L1 introduz escassez para zerar alguns dos parâmetros de peso. A regularização de L2 penalizará os parâmetros de pesos sem torná-los escassos - uma razão pela qual L2 é mais comum.\n",
        "\n",
        "O que estamos usando para aplicar a regularização:\n",
        "- `kernel_regularizer`: função reguladora aplicada à matriz de pesos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HFGmcwduwVyQ",
        "colab": {}
      },
      "source": [
        "# Modelo\n",
        "l2_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n",
        "                       activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Configuramos o modelo para treinamento\n",
        "l2_model.compile(optimizer='adam',\n",
        "                 loss='binary_crossentropy',\n",
        "                 metrics=['accuracy', 'binary_crossentropy'])\n",
        "# Treino\n",
        "l2_model_history = l2_model.fit(train_data, train_labels,\n",
        "                                epochs=20,\n",
        "                                batch_size=512,\n",
        "                                validation_data=(test_data, test_labels),\n",
        "                                verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wkfLyxBZdh_",
        "colab": {}
      },
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('l2', l2_model_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Kx1YHMsVxWjP"
      },
      "source": [
        "Como você pode ver, o modelo regularizado L2 tornou-se muito mais resistente à adaptação excessiva do que o modelo de linha de base, embora ambos os modelos tenham o mesmo número de parâmetros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HmnBNOOVxiG8"
      },
      "source": [
        "### Adicionar dropout\n",
        "\n",
        "O abandono é uma das técnicas de regularização mais eficazes e mais usadas para redes neurais, desenvolvida por Hinton e seus alunos na Universidade de Toronto. A desistência, aplicada a uma camada, consiste em \"desistir\" aleatoriamente (ou seja, zerar) uma série de recursos de saída da camada durante o treinamento. Digamos que uma determinada camada retornaria normalmente um vetor [0,2; 0,5; 1,3; 0,8; 1,1] para uma determinada amostra de entrada durante o treinamento; depois de aplicar o dropout, esse vetor terá algumas entradas zero distribuídas aleatoriamente, por exemplo, [0, 0,5, 1,3, 0, 1,1]. A \"taxa de desistência\" é a fração dos recursos que estão sendo zerados; geralmente é definido entre 0,2 e 0,5. No momento do teste, nenhuma unidade é eliminada e, em vez disso, os valores de saída da camada são reduzidos por um fator igual à taxa de abandono.\n",
        "\n",
        "Estamos usando:\n",
        "- [`keras.layers.Dropout`](https://keras.io/layers/core/#dropout): aplica _Dropout_ no _input_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OFEYvtrHxSWS",
        "colab": {}
      },
      "source": [
        "# Modelo\n",
        "dpt_model = keras.models.Sequential([\n",
        "    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# Configuramos o modelo para treinamento\n",
        "dpt_model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy','binary_crossentropy'])\n",
        "# Treino\n",
        "dpt_model_history = dpt_model.fit(train_data, train_labels,\n",
        "                                  epochs=20,\n",
        "                                  batch_size=512,\n",
        "                                  validation_data=(test_data, test_labels),\n",
        "                                  verbose=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SPZqwVchx5xp",
        "colab": {}
      },
      "source": [
        "plot_history([('baseline', baseline_history),\n",
        "              ('dropout', dpt_model_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gjfnkEeQyAFG"
      },
      "source": [
        "A adição de desistências é uma clara melhoria em relação ao modelo de linha de base."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y71OWFH9E9UD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}