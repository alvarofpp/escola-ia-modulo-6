{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "activation-functions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54SpatUbyc6v",
        "colab_type": "text"
      },
      "source": [
        "# Funções de ativação\n",
        "Quando você constrói sua rede neural, uma das escolhas que você tem que fazer é qual a função de ativação usar nas camadas ocultas, bem como quais serão as unidades de saída de sua rede neural.\n",
        "\n",
        "Funções de ativação devem atender a dois critérios:\n",
        "1.  Deve estar ativa para entradas \"corretas\" e inativa para entradas \"erradas\". Os valores referentes a cada classificação variam;\n",
        "2.  Deve ser não linear para que a rede como um todo possa representar funções não-lineares.\n",
        "\n",
        "Elas devem serem não lineares devido a esse comportamento fazer com que as redes neurais aprendam mais do que relações lineares entre as variáveis dependentes e independentes. O problema que essa não linearidade pode trazer é a de fazer com que a superfície de custo da rede neural deixe de ser convexa, dessa forma tornando mais complicado o processo de otimização. Além disso, algumas não linearidade tornam o problema de gradientes explodindo ou desvanecendo mais evidente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsNwlaMwyoN_",
        "colab_type": "text"
      },
      "source": [
        "## Função sigmóide\n",
        "Idealmente se usa essa ativação apenas nas camadas de saída e em problemas de classificação binária, visto que outras funções possuem comportamentos melhores de apredizagem para as outras camadas.\n",
        "\n",
        "Sua equação é:\n",
        "\\begin{equation}\n",
        "    \\sigma(x) = \\frac{1}{1+e^{-x}}\n",
        "\\end{equation}\n",
        "\n",
        "E sua derivada é:\n",
        "\\begin{equation}\n",
        "    \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))\n",
        "\\end{equation}\n",
        "\n",
        "O gráfico a seguir mostra o comportamento dela ($z$ nesse caso seria o eixo horizontal):\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\">\n",
        "\n",
        "Podemos perceber, por exemplo, que ela cruza o eixo vertical em $0.5$.\n",
        "\n",
        "Podemos realizar algumas observações, como:\n",
        "1.  Se $z$ é muito grande, então $e^{-z}$ será próximo de zero. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+0} \\thickapprox 1$;\n",
        "1.  Se $z$ é muito pequeno ou é um número negativo muito grande, então $e^{-z}$ será um número muito grande. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+numero\\_grande} \\thickapprox 0$;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhiXIXZzzM7X",
        "colab_type": "text"
      },
      "source": [
        "## Função TanH\n",
        "Também conhecida como **função tangente hiperbólica**, é uma função que varia de $-1$ a $+1$. Comparado a função sigmóide, a função TanH possui a característica de melhor centralizar os dados, porque a média das ativações está mais próxima à $0$ (podendo atingir esse valor) do que a $0,5$, como é o caso da função sigmóide. Essa característica facilita o aprendizado para a próxima camada.\n",
        "\n",
        "Sua equação é:\n",
        "\\begin{equation}\n",
        "    tanh(x) = \\frac{e^{x}-e^{-x}}{e^{x}+e^{-x}}\n",
        "\\end{equation}\n",
        "\n",
        "Se você perceber, ela é apenas como se tivessemos movido a função sigmóide. Devido a isso, podemos reescrever a função como:\n",
        "\n",
        "Sua equação é:\n",
        "\\begin{equation}\n",
        "    tanh(x) = 2\\sigma(2x) - 1\n",
        "\\end{equation}\n",
        "\n",
        "Sua sua derivada é:\n",
        "\\begin{equation}\n",
        "    tanh'(x) = 1 - tanh^{2}(x)\n",
        "\\end{equation}\n",
        "\n",
        "O gráfico a seguir mostra o comportamento dela ($x$ nesse caso seria o eixo horizontal):\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/87/Hyperbolic_Tangent.svg/1200px-Hyperbolic_Tangent.svg.png\">\n",
        "\n",
        "Podemos perceber, por exemplo, que ela cruza o eixo vertical em $0$.\n",
        "\n",
        "No geral, a função tanH é melhor que a função sigmóide, com exceção para a camada de saída, porque se $y = 0$ ou $y = 1$, nesse caso as saídas ($\\hat{y}$) irão variar entre $0$ e $1$, e não entre $-1$ e $1$. Porém, uma desvantagem que ambas as funções compartilham é quando $x$ é muito grande ou muito pequeno, então o gradiente, ou a derivada ou a inclinação dessa função torna-se muito pequena, sendo próxima de $0$, podendo retardar a descida do gradiente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDE7HK2htLwY",
        "colab_type": "text"
      },
      "source": [
        "## Função ReLU\n",
        "Também conhecida como **função linear retificada**. Sua equação é:\n",
        "\\begin{equation}\n",
        "    ReLU(x) = max\\{0, x\\}\n",
        "\\end{equation}\n",
        "\n",
        "Sua derivada é:\n",
        "\\begin{equation}\n",
        "    ReLU'(x) = \\left\\{\n",
        "      \\begin{array}{ll}\n",
        "        1, x \\geq 0 \\\\\n",
        "        0, c.c.\n",
        "      \\end{array}\n",
        "    \\right.\n",
        "\\end{equation}\n",
        "\n",
        "O gráfico a seguir mostra o comportamento dela ($x$ nesse caso seria o eixo horizontal):\n",
        "\n",
        "<img src=\"https://matheusfacure.github.io/img/tutorial/activations/RELU.png\">\n",
        "\n",
        "Algo importante a ser ressaltado é que, visto que essa função se parece com a função identidade, ela torna-se mais fácil de ser otimizada que outras funções. Outra observação que podemos fazer é que sua derivada não está definida em $0$, mas podemos implementá-la como sendo $0$ ou $1$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJRpHGXQzJg9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}