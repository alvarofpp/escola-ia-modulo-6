{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT4d8uPWmWBy"
   },
   "source": [
    "# Notação\n",
    "Notação matemática para deep learning.\n",
    "\n",
    "## Tamanhos\n",
    "| Notação | Significado |\n",
    "| -------------- | ------------------ |\n",
    "| $m$ | Número de exemplos no dataset |\n",
    "| $n_{x}$ | Tamanho da entrada  |\n",
    "| $n_{y}$ | Tamanho da saída (ou número de classes)  |\n",
    "| $n_{h}^{[l]}$ | Número de unidades ocultas da camada $l^{th}$ |\n",
    "| $L$ | Número de camadas na rede |\n",
    "\n",
    "**Observação**: Em um _loop_, é possível denotar $n_{x} = n_{h}^{[0]}$ e $n_{y} = n_{h}^{numero\\_de\\_camadas + 1}$.\n",
    "\n",
    "## Objetos\n",
    "| Notação | Significado |\n",
    "| -------------- | ------------------ |\n",
    "| $X \\in \\mathbb{R}^{n_{x} \\times m}$ | Matriz de entrada |\n",
    "| $x^{(i)} \\in \\mathbb{R}^{n_{x}}$ | Representa o vetor da columa $i^{th}$  |\n",
    "| $Y \\in \\mathbb{R}^{n_{y} \\times m}$ | Matriz de rótulos |\n",
    "| $y^{(i)} \\in \\mathbb{R}^{n_{y}}$ | Rótulo de saída para o exemplo $i^{th}$  |\n",
    "| $W^{[l]} \\in \\mathbb{R}^{numero\\_de\\_unidades\\_na\\_proxima\\_camada \\times numero\\_de\\_unidades\\_na\\_camada\\_anterior}$ | É a matriz de peso, $[l]$ indica a camada |\n",
    "| $b^{[l]} \\in \\mathbb{R}^{numero\\_de\\_unidades\\_na\\_proxima\\_camada}$ | É o vetor de viés na camada $l^{th}$ |\n",
    "| $\\hat{y} \\in \\mathbb{R}^{n_{y}}$ | É o vetor de saída previsto. Também pode ser denotado como $a^{[L]}$, onde $L$ é o número de camadas na rede |\n",
    "\\begin{equation}\n",
    "X_{n_{x} \\times m} =\n",
    "  \\begin{bmatrix}\n",
    "    \\vdots & \\vdots &\\vdots &   & \\vdots \\\\\n",
    "    x^{(1)} & x^{(2)} & x^{(3)} & \\dots  & x^{(m)} \\\\\n",
    "    \\vdots & \\vdots &\\vdots &   & \\vdots\n",
    "  \\end{bmatrix}\n",
    "Y_{1 \\times m} =\n",
    "  \\begin{bmatrix}\n",
    "    y^{(1)} & y^{(2)} & y^{(3)} & \\dots  & y^{(m)}\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "## Outros\n",
    "| Notação | Significado |\n",
    "| -------------- | ------------------ |\n",
    "| $(x^{(1)}, y^{(1)})$ | Exemplo de treinamento único |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BHb1xaUR4lAt"
   },
   "source": [
    "# Regressão Logística\n",
    "Regressão logística é um algoritmo de classificação binária, isso quer dizer que é um algoritmo de aprendizagem que usamos quando os rótulos de saída $Y$ em um problema de aprendizado supervisionado são ou $0$ ou $1$.\n",
    "\n",
    "Dado um vetor de característica $x$, nós queremos prever $\\hat{y}$, que é a probabilidade de $y$ ser igual a $1$. Em outras palavras: se $x$ é uma imagem, $\\hat{y}$ nos dirá se o objeto que estamos procurando está na imagem ou não.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\underbrace{P(y = 1|x)}_{0 \\leq \\hat{y} \\leq 1}\n",
    "\\end{equation}\n",
    "\n",
    "Os parâmetros de regressão logística serão: $w$, que é um vetor de dimensão $n_{x}$, e $b$, que é um número real.\n",
    "\n",
    "\\begin{equation}\n",
    "    w \\in \\mathbb{R}^{n_{x}} \\\\\n",
    "    b \\in \\mathbb{R}\n",
    "\\end{equation}\n",
    "\n",
    "Se estivessemos usando regressão linear, poderiamos declarar que $\\hat{y} = w^{t} \\times x + b$. O problema é que essa equação pode resultar em números muito maiores que 1 ou até mesmo números negativos, o que não é nada correto visto que nosso problema é de classificação binária. Devido isso, aplicamos a função sigmóide na equação anterior (chamaremos a equação anterior de $z$):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\hat{y} = \\sigma(\\underbrace{w^{t} \\times x + b}_{z})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yeuojpMkFvh9"
   },
   "source": [
    "## Função sigmóide\n",
    "A função sigmóide é uma função de ativação, isso quer dizer que ela atende a dois critérios:\n",
    "1.  Deve estar ativa (próxima de $+1$) para entradas \"corretas\" e inativa (próxima a $0$) para entradas \"erradas\";\n",
    "2.  Deve ser não linear para que a rede como um todo possa representar funções não-lineares.\n",
    "\n",
    "O gráfico a seguir mostra o comportamento dela ($z$ nesse caso seria o eixo horizontal):\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\">\n",
    "\n",
    "Podemos perceber, por exemplo, que ela cruza o eixo vertical em $0.5$.\n",
    "\n",
    "Sua equação é:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "Podemos realizar algumas observações, como:\n",
    "1.  Se $z$ é muito grande, então $e^{-z}$ será próximo de zero. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+0} \\thickapprox 1$;\n",
    "1.  Se $z$ é muito pequeno ou é um número negativo muito grande, então $e^{-z}$ será um número muito grande. Dessa forma: $\\sigma(z) \\thickapprox \\frac{1}{1+numero\\_grande} \\thickapprox 0$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zy-29dtFI4Il"
   },
   "source": [
    "## Função de custo\n",
    "Serve para medir o quão bem os seus parâmetros $w$ e $b$ estão perfomando no conjunto de treinamento. Consiste em ser a média da função de perda (que pode ser vista logo mais abaixo) de cada exemplo treinado.\n",
    "\\begin{equation}\n",
    "    J(w,b) = \\frac{1}{m}\\sum_{i=1}^{m} \\mathcal{L}(\\hat{y}^{i}, y^{i}) = -\\frac{1}{m}\\sum_{i=1}^{m} \\begin{bmatrix}\n",
    "    y^{i} \\log \\hat{y}^{i} + (1-y^{i})\\log(1-\\hat{y}^{i})\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "### Função de perda\n",
    "Também chamada de **função de erro**, é usada para medir o quão boa é a saída prevista $\\hat{y}$, quando o rótulo verdadeiro for $y$. Podemos definir a perda quando o algoritmo dá o resultado, $\\hat{y}$, em relação ao rótulo verdadeiro, $y$, como:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\hat{y}, y) = \\frac{1}{2}(\\hat{y}-y)^{2}\n",
    "\\end{equation}\n",
    "\n",
    "O problema dessa equação acima é que nos deparamos com o problema de otimização com múltiplas ótimos saídas, pois não será convexo. Devido a isso, nossa função de perda deverá ser:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(\\hat{y}, y) = (y \\log \\hat{y} + (1-y)\\log(1-\\hat{y}))\n",
    "\\end{equation}\n",
    "\n",
    "Essa equação acima faz sentido se observarmos dois casos: $y=0$ e $y=1$.\n",
    "\n",
    "1.  $y=1 \\Longrightarrow \\mathcal{L}(\\hat{y}, y) = - \\log \\hat{y}$, quanto mais $\\hat{y}$ se aproxima de $0$, $- \\log \\hat{y}$ cresce exponencialmente para o infinito. Porém a função sigmóide não permite que seja maior que $1$, portanto será próximo de $1$.\n",
    "1.  $y=0 \\Longrightarrow \\mathcal{L}(\\hat{y}, y) = - \\log(1-\\hat{y})$, quanto mais $\\hat{y}$ se distancia de $0$, $\\log(1-\\hat{y})$ vai para infinito. Vale notar o sinal $-$ no início da equação. Logo, como a função sigmóide não permite ser menor que $0$, $\\hat{y}$ será próximo de $0$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoDAknnewR9_"
   },
   "source": [
    "## Derivação\n",
    "A derivada de uma função $f(x) = y$, num ponto $x = x_{0}$, é igual ao valor da tangente trigonométrica do ângulo formado pela tangente geométrica à curva representativa de $f(x) = y$, no $x = x_{0}$, ou seja, a derivada é o coeficiente angular da reta tangente ao gráfico da função no ponto $x_{0}$. Em outras palavras, podemos dizer que a derivada em um ponto de função $f(x) = y$ representa a taxa de variação instantânea do $y$ em relação a $x$ neste ponto.\n",
    "\n",
    "### Algumas propriedades\n",
    "Agora iremos ver algumas propriedades da derivação.\n",
    "\n",
    "- i) Se $f(x) = a$, então $f'(x) = 0$\n",
    "- ii) Se $f(x) = ax$, então $f'(x) = a$\n",
    "- iii) (Regra do tombo) Se $f(x) = x^{a}$, então $f'(x) = a \\times x^{a-1}$\n",
    "- iv) (Derivada da soma) $\\begin{bmatrix}f(x) + g(x)\\end{bmatrix}' = f'(x) + g'(x)$\n",
    "- v) $[a \\times f(x)]' = a \\times f'(x)$\n",
    "- vi) (Regra do produto) $[f(x) \\times g(x)]' = f(x)' \\times g(x) + f(x) \\times g(x)'$\n",
    "- vii) (Regra do quociente) $\\begin{bmatrix}\\frac{f(x)}{g(x)}\\end{bmatrix}'\n",
    "= \\frac{f'(x) \\times g(x) - f(x) \\times g'(x)}{\\begin{bmatrix}g(x)\\end{bmatrix}^{2}}$\n",
    "- viii) (Regra da cadeia) $f$ e $g$ são funções, então $\\frac{d}{dx}(f \\circ g) = \\frac{df}{dg} \\times \\frac{dg}{dx}$\n",
    "\n",
    "### Algumas derivadas básicas\n",
    "\n",
    "- Derivada de uma constante: $\\frac{d}{dx}(c) = 0$\n",
    "- Derivada de soma/subtração: $\\frac{d}{dx}(z \\pm w) = \\frac{dz}{dx} \\pm \\frac{dw}{dx}$\n",
    "- Produto por uma constante: $\\frac{d}{dx}(c \\times w) = c\\frac{dw}{dx}$\n",
    "- Derivada do produto: $\\frac{d}{dx}(z \\times w) = z\\frac{dw}{dx} + w\\frac{dz}{dx}$\n",
    "- Derivada da divisão:  $\\frac{d}{dx}(\\frac{z}{w})\n",
    "= \\frac{w\\frac{dz}{dx} - z\\frac{dw}{dx}}{w^{2}}$\n",
    "- Derivada de $ln$: $f(x) = ln(x)$, então $f'(x) = \\frac{1}{x}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7TCjfaggfKBi"
   },
   "source": [
    "## Gradiente decrescente\n",
    "Trata-se de um método iterativo de otimização, com ele iremos aprender os parâmetros $w$ e $b$ do conjunto de treinamento.\n",
    "\n",
    "Inicialmente atribuímos valores para $w$ e $b$, normalmente se atribui $0$, mas também pode ser valores aleatórios. Após isso será cálculado um valor referente a $w$ e $b$ que servirá para indicar a direção da descida mais íngreme ou descer o mais rápido possível. Esse passo será repetida várias vezes até convergir ao ponto ideal ou chegue o mais próximo dele.\n",
    "\n",
    "\\begin{equation}\n",
    "      w := w - \\alpha \\frac{\\partial J(w, b)}{\\partial w} \\\\\n",
    "      b := b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} \\\\\n",
    "\\end{equation}\n",
    "\n",
    "Onde:\n",
    "- $\\alpha$ - É a taxa de aprendizagem e controla o quão grande será o passo tomado em cada iteração do gradiente decrescente\n",
    "- $\\frac{\\partial J(w, b)}{\\partial w}$ - Derivada de $w$, representa a atualização ou mudança desejada no parâmetro $w$. O mesmo vale para $\\frac{\\partial J(w, b)}{\\partial b}$.\n",
    "\n",
    "Vale lembrar que a derivada é a inclinação de uma função no ponto. Dessa forma:\n",
    "- Se a derivada for positiva, $w$ sobre uma subtração;\n",
    "- Se a derivada for negativa, $w$ sobre uma adição;\n",
    "- Se a derivada der zero, significa que chegamos no ponto crítico da função. Por se tratar de uma função convexa, existe apenas um único ponto crítico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dai3Y3YN8dxf"
   },
   "source": [
    "## Implementação\n",
    "Considere $a = \\hat{y}$.\n",
    "\n",
    "### Não vetorizada\n",
    "Teriamos inicialmente as variáveis inicializadas:\n",
    "\\begin{equation}\n",
    "  J = 0, dw_{1} = 0, dw_{2} = 0, db = 0\n",
    "\\end{equation}\n",
    "\n",
    "Depois declaramos um *loop* de $1$ até $m$. Seria algo como `for i = 1 to m`. Dentro desse laço, teriamos essa sequência de cálculos:\n",
    "\\begin{equation}\n",
    "  z^{(i)} = w^{t} \\times x^{(i)} + b \\\\\n",
    "  a^{(i)} = \\sigma(z^{(i)}) \\\\\n",
    "  J += -\\begin{bmatrix}y^{(i)} \\log a^{(i)} + (1-y^{(i)})\\log(1-a^{(i)})\\end{bmatrix} \\\\\n",
    "  dz^{(i)} = a^{(i)}-y^{(i)} \\\\\n",
    "  dw_{1} += x_{1}^{(i)} \\times dz^{(i)} \\\\\n",
    "  dw_{2} += x_{2}^{(i)} \\times dz^{(i)} \\\\\n",
    "  db += dz^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "E, após todo o laço, dividimos os valores pelo total de *loops* que realizamos:\n",
    "\\begin{equation}\n",
    "  J = \\frac{J}{m}, dw_{1} = \\frac{dw_{1}}{m}, dw_{2} = \\frac{dw_{2}}{m}, db = \\frac{db}{m}\n",
    "\\end{equation}\n",
    "\n",
    "### Vetorizada\n",
    "Para implementar, iremos usar a biblioteca [*numpy*](https://www.numpy.org/), assim não precisamos realizar nenhum *loop*. Dessa forma teriamos:\n",
    "\\begin{equation}\n",
    "  z = np.dot(w.T, x) + b \\\\\n",
    "  a = \\sigma(z^{(i)}) \\\\\n",
    "  dz = a - y \\\\\n",
    "  dw =\\frac{1}{m} \\times dz^{t} \\\\\n",
    "  db =\\frac{1}{m} \\times np.sum(dz) \\\\\n",
    "  w := w - \\alpha \\times dw \\\\\n",
    "  b := b - \\alpha \\times db\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [numpy](http://www.numpy.org/) é o pacote fundamental para computação científica com Python;\n",
    "- [h5py](http://www.h5py.org/) é um pacote comum para interagir com um conjunto de dados armazenado em um arquivo H5;\n",
    "- [matplotlib](https://matplotlib.org/) é uma biblioteca para construir gráficos em Python;\n",
    "- [PIL](http://www.pythonware.com/products/pil/) ou [scipy](https://www.scipy.org/) poderm ser usados para testar seu modelo com outras fotos no final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:15.764399Z",
     "start_time": "2019-04-28T19:43:05.702977Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-NRiQVcs8lEr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yRR55yzHcAHJ"
   },
   "source": [
    "Primeiro vamos carregar os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:22.073544Z",
     "start_time": "2019-04-28T19:43:22.067561Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NWJ4ZLt7cAOY"
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:23.354934Z",
     "start_time": "2019-04-28T19:43:22.936829Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Yl8x8PS4cngg"
   },
   "outputs": [],
   "source": [
    "# Loading the data (cat/non-cat)\n",
    "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DzZUjdIKcKig"
   },
   "source": [
    "Agora precisamos garantir as dimensões dos dados, além de padronizá-los:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:24.209672Z",
     "start_time": "2019-04-28T19:43:24.193685Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NdZDzcuxcKp1"
   },
   "outputs": [],
   "source": [
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "# standardize dataset\n",
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c6xEFs9oc-nh"
   },
   "source": [
    "Vamos agora implementar o algoritmo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:25.221651Z",
     "start_time": "2019-04-28T19:43:25.202702Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "SGJ9T5g6B4uE"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "  \"\"\"Compute the sigmoid of x.\"\"\"\n",
    "  return 1/(1+np.exp(-x))\n",
    "\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\"\"\"\n",
    "    \n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0.\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"Implement the cost function and its gradient for the propagation explained above.\"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Encontrar o custo (forward propagation)\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    cost = -(1/m)*np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "    \n",
    "    # Encontrar o gradiente (backward propagation)\n",
    "    dw = (np.dot(X, (A- Y).T))/m\n",
    "    db = (np.sum(A - Y))/m\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"This function optimizes w and b by running a gradient descent algorithm.\"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "def predict(w, b, X):\n",
    "    '''Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b).'''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0,i] = A[0,i] >= 0.5\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"Builds the logistic regression model by calling the function you've implemented previously.\"\"\"\n",
    "    \n",
    "    # Initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pP4dLu_KdSWQ"
   },
   "source": [
    "Vamos executar para vermos se está tudo funcionando como deveria:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:34.770572Z",
     "start_time": "2019-04-28T19:43:28.612432Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "id": "zGc6NeMDdSiA",
    "outputId": "b29b8585-aa89-46d5-fa56-38c097890cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693147\n",
      "Cost after iteration 100: 0.584508\n",
      "Cost after iteration 200: 0.466949\n",
      "Cost after iteration 300: 0.376007\n",
      "Cost after iteration 400: 0.331463\n",
      "Cost after iteration 500: 0.303273\n",
      "Cost after iteration 600: 0.279880\n",
      "Cost after iteration 700: 0.260042\n",
      "Cost after iteration 800: 0.242941\n",
      "Cost after iteration 900: 0.228004\n",
      "Cost after iteration 1000: 0.214820\n",
      "Cost after iteration 1100: 0.203078\n",
      "Cost after iteration 1200: 0.192544\n",
      "Cost after iteration 1300: 0.183033\n",
      "Cost after iteration 1400: 0.174399\n",
      "Cost after iteration 1500: 0.166521\n",
      "Cost after iteration 1600: 0.159305\n",
      "Cost after iteration 1700: 0.152667\n",
      "Cost after iteration 1800: 0.146542\n",
      "Cost after iteration 1900: 0.140872\n",
      "train accuracy: 99.04306220095694 %\n",
      "test accuracy: 70.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fX0Vp7b7Z9gI"
   },
   "source": [
    "Exemplo de código para ajudar a escolher a taxa de apredizagem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-28T19:43:48.059532Z",
     "start_time": "2019-04-28T19:43:34.784535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "colab_type": "code",
    "id": "1M-48UX-7Bz-",
    "outputId": "eb016d11-6b74-4337-abcc-ba785365fb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate is: 0.01\n",
      "train accuracy: 99.52153110047847 %\n",
      "test accuracy: 68.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.001\n",
      "train accuracy: 88.99521531100478 %\n",
      "test accuracy: 64.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n",
      "learning rate is: 0.0001\n",
      "train accuracy: 68.42105263157895 %\n",
      "test accuracy: 36.0 %\n",
      "\n",
      "-------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "models = {}\n",
    "for i in learning_rates:\n",
    "    print (\"learning rate is: \" + str(i))\n",
    "    models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False)\n",
    "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
    "\n",
    "for i in learning_rates:\n",
    "    plt.plot(np.squeeze(models[str(i)][\"costs\"]), label= str(models[str(i)][\"learning_rate\"]))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (hundreds)')\n",
    "\n",
    "legend = plt.legend(loc='upper center', shadow=True)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('0.90')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir serve para testar o modelo final com outras imagens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LjH0FPoLdZOS"
   },
   "outputs": [],
   "source": [
    "my_image = \"my_image.jpg\"\n",
    "\n",
    "# We preprocess the image to fit your algorithm.\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], my_image)\n",
    "\n",
    "plt.imshow(image)\n",
    "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "logistic-regression.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
